################################################################################
###
###   O B S O L E T E    F I L E    T O    B E   R E M O V E D 
###
################################################################################


## 
## import numpy as np
## from sklearn.ensemble import GradientBoostingClassifier
## 
## def _singletree (tree, node):
##   if tree.feature [node] >= 0:
##     return "(x%02d <= %f ? %s : %s)" % (tree.feature[node], 
##         tree.threshold[node],
##         _singletree ( tree, tree.children_left[node] ), 
##         _singletree ( tree, tree.children_right[node] ) )  
##   else:
##     return str(tree.value [node][0][0]) 
## 
##   
## 
## 
## 
## 
## def tree2C ( forest ):
##   """
##   Converts a trained sklearn tree into a C function. 
##   """
##   n_classes = forest.n_classes_ if forest.n_classes_ > 2 else 1 
##   lines = ["/* File autogenerated from TrackPar package. DO NOT EDIT! */"]
## 
##   if n_classes > 1: 
##     for iClass in range(n_classes):
##       lines.append ( "/*  ret [ %d ]   is the probability for category:  %-15s */" %
##           ( iClass,  str(forest.classes_[iClass]) ) )
##   
##   retvar = "double ret[%d]" % n_classes 
##   lines += [
##       "#include <math.h>",
##       "double *response (%s, %s)" % (retvar, ", ".join(["double x%02d" % d for d in range(forest.n_features_)])),
##       "{", 
##       "  for (short i=0; i < %d; ++i) ret[i] = 0.f;" % n_classes, 
##       "  double %s;" % (", ".join("y%02d" % d for d in range(n_classes)) )
##     ]
## 
##   for iTree, tree in enumerate(forest.estimators_):
##     lines += [" /** TREE %03d **/" % iTree ]
##     for iClass in range(n_classes):
##       lines += [
##          "  y%02d = %s; " % ( iClass, _singletree(tree[iClass].tree_,0) ) , 
##          #"  if (y%02d > 1e30) y%02d = 1e30;" % ( iClass, iClass) , 
##        ]
##     for iClass in range(n_classes):
##       lines += [
##          "  ret[%d] += %f * y%02d; " % ( iClass, forest.learning_rate, iClass) 
##          #"  ret[%d] += y%02d/(%s); " % ( iClass, iClass, " + ".join (["y%02d"%d for d in range(forest.n_classes_) ]) )
##        ]
## 
##   lines += [
##       "  short argmax = 0; ", 
##       "  for (int i = 0; i < %d; ++i) if (ret[i] > ret[argmax]) argmax = i; " % n_classes, 
##       "  if (ret[argmax] > 1e10) { " ,
##       "    for (int i = 0; i < %d; ++i) ret[i] = (i==argmax ? 1.: 0.); " % n_classes, 
##       "    return ret; ",
##       "  }", 
##       "  for (short i=0; i < %d; ++i) ret[i] = exp(ret[i]);" % n_classes, 
##       "  for (short i=0; i < %d; ++i) ret[i] = (ret[i] > 1e300?1e300:ret[i]);" % n_classes, 
##       "  long double sum = 0;", 
##       "  for (short i=0; i < %d; ++i) sum += ret[i];" % n_classes, 
##       "  for (short i=0; i < %d; ++i) ret[i] /= sum;" % n_classes, 
##     ] 
## 
## 
## 
## 
## 
##   lines += ["  return ret;", "}"]
## 
##   return "\n".join ( lines ) 
## 
## if __name__ == '__main__':
##   import pickle 
##   with open ( '../trkEfficiency_tracking.pkl', 'rb') as fin:
##     classifier = pickle.load(fin)
## 
##   from sklearn.ensemble._gradient_boosting import predict_stages 
##   f = np.load ( "efficiencyBdtDataset.npz" )
##   X = f['training'][:5]
##   print ("INPUT:", X) 
##   #X = np.array([[26.97, -95.27, 1954.96, 2.73, 8081.56, -1211.09, 1954.96, 0.56]] ) 
##   raw_predictions = classifier._raw_predict_init(X)
##   predict_stages(classifier.estimators_, X, classifier.learning_rate,raw_predictions)
## 
##   print (raw_predictions) 
## 
##   proba = (classifier.decision_function (np.array (X)))
##   print ("DECISION: ", proba)
##   proba = (classifier.predict_proba (np.array (X)))
##   print ("PROBA: ", proba)
## 
##   print (classifier.classes_) 
##   #print (tree2C(classifier))
##   with open ( 'my_beautiful_bdt.C', 'w' ) as fout:
##     fout.write ( tree2C(classifier) ) 
##     
## 

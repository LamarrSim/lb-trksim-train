{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8193534e-829a-4c24-b72f-8129cd6f98e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 21:18:31.188075: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "!conda run -n base python3 -m pip install -q git+https://github.com/landerlini/FastQuantileLayer.git --force-reinstall --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1786ea4-e8ef-4f26-9e83-11b9afd9db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 21:18:38.324475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.experimental.load(\"./acceptance-train.tfrecords\")\n",
    "validation_dataset = tf.data.experimental.load(\"./acceptance-validation.tfrecords\").batch(1000)\n",
    "\n",
    "X, y = next(iter(train_dataset.batch(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c192b02-e513-47fb-98a3-8d1d955fece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fast_quantile_layer (FastQu  (None, 10)               0         \n",
      " antileLayer)                                                    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,145\n",
      "Trainable params: 150,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from FastQuantileLayer import FastQuantileLayer\n",
    "layers = [FastQuantileLayer(output_distribution='normal', trainable=True).fit(X)]\n",
    "layers += [tf.keras.layers.Dense(128, activation='tanh', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.L2(0.01)) for _ in range(10)]\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
    "model.build(input_shape = [None, X.shape[1]])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8c457b-076b-4340-996f-695ae7598c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134972ba-596f-45fc-a754-22ab990fcf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 9s 940ms/step - loss: 20.8800 - val_loss: 13.3057\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 6s 815ms/step - loss: 10.6766 - val_loss: 7.4596\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 6s 846ms/step - loss: 6.1736 - val_loss: 4.9528\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 7s 898ms/step - loss: 4.0063 - val_loss: 2.8664\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 6s 818ms/step - loss: 2.3958 - val_loss: 1.7927\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 7s 878ms/step - loss: 1.5166 - val_loss: 1.2215\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 6s 761ms/step - loss: 1.0864 - val_loss: 0.9511\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 6s 847ms/step - loss: 0.8780 - val_loss: 0.8165\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 6s 862ms/step - loss: 0.7722 - val_loss: 0.7447\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 6s 799ms/step - loss: 0.7140 - val_loss: 0.7028\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 6s 884ms/step - loss: 0.6792 - val_loss: 0.6793\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 6s 810ms/step - loss: 0.6595 - val_loss: 0.6633\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 7s 902ms/step - loss: 0.6498 - val_loss: 0.6619\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 6s 898ms/step - loss: 0.6506 - val_loss: 0.6676\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 7s 949ms/step - loss: 0.6621 - val_loss: 0.6833\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 6s 874ms/step - loss: 0.6695 - val_loss: 0.6792\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 7s 914ms/step - loss: 0.6693 - val_loss: 0.7458\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 7s 890ms/step - loss: 1.2264 - val_loss: 1.5335\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=BinaryCrossentropy(label_smoothing=0.01), optimizer=RMSprop(1e-2))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "data = train_dataset.batch(100_000).prefetch(tf.data.AUTOTUNE)\n",
    "history = model.fit(data, epochs=50, validation_data=next(iter(validation_dataset)), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65cec0c-e525-4d8f-a0bc-127f6bd10347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 12s 942ms/step - loss: 1.0981 - val_loss: 0.8947\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 6s 848ms/step - loss: 0.8344 - val_loss: 0.7813\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 7s 888ms/step - loss: 0.7424 - val_loss: 0.7131\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 6s 877ms/step - loss: 0.6870 - val_loss: 0.6770\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 6s 843ms/step - loss: 0.6579 - val_loss: 0.6585\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 6s 864ms/step - loss: 0.6427 - val_loss: 0.6492\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 6s 817ms/step - loss: 0.6349 - val_loss: 0.6448\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 6s 760ms/step - loss: 0.6312 - val_loss: 0.6427\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 6s 827ms/step - loss: 0.6295 - val_loss: 0.6418\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 6s 862ms/step - loss: 0.6288 - val_loss: 0.6414\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 6s 829ms/step - loss: 0.6285 - val_loss: 0.6413\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 7s 895ms/step - loss: 0.6284 - val_loss: 0.6413\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 7s 886ms/step - loss: 0.6284 - val_loss: 0.6413\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 7s 860ms/step - loss: 0.6285 - val_loss: 0.6413\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 7s 885ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 6s 827ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 6s 842ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 6s 775ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 6s 848ms/step - loss: 0.6285 - val_loss: 0.6416\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 7s 904ms/step - loss: 0.6286 - val_loss: 0.6413\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 6s 849ms/step - loss: 0.6286 - val_loss: 0.6414\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 6s 806ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 7s 904ms/step - loss: 0.6286 - val_loss: 0.6414\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 6s 857ms/step - loss: 0.6286 - val_loss: 0.6414\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 6s 849ms/step - loss: 0.6285 - val_loss: 0.6414\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 6s 865ms/step - loss: 0.6286 - val_loss: 0.6414\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 7s 922ms/step - loss: 0.6286 - val_loss: 0.6415\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 6s 783ms/step - loss: 0.6286 - val_loss: 0.6413\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 7s 851ms/step - loss: 0.6286 - val_loss: 0.6415\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 6s 866ms/step - loss: 0.6285 - val_loss: 0.6414\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=BinaryCrossentropy(label_smoothing=0.00), optimizer=RMSprop(1e-3))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "data = train_dataset.batch(100_000).prefetch(tf.data.AUTOTUNE)\n",
    "history_ft = model.fit(data, epochs=50, validation_data=next(iter(validation_dataset)), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a5d610-777b-4c14-b3d8-f84b39ae4ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'] + history_ft.history['loss'], label=\"Loss (train)\")\n",
    "plt.plot(history.history['val_loss'] + history_ft.history['val_loss'], label=\"Loss (validation)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary cross-entropy\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e35b722-1b03-467a-a909-8518cb6ce480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RMSprop in module keras.optimizers.optimizer_v2.rmsprop:\n",
      "\n",
      "class RMSprop(keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2)\n",
      " |  RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n",
      " |  \n",
      " |  Optimizer that implements the RMSprop algorithm.\n",
      " |  \n",
      " |  The gist of RMSprop is to:\n",
      " |  \n",
      " |  - Maintain a moving (discounted) average of the square of gradients\n",
      " |  - Divide the gradient by the root of this average\n",
      " |  \n",
      " |  This implementation of RMSprop uses plain momentum, not Nesterov momentum.\n",
      " |  \n",
      " |  The centered version additionally maintains a moving average of the\n",
      " |  gradients, and uses that average to estimate the variance.\n",
      " |  \n",
      " |  Args:\n",
      " |    learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
      " |      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      learning rate. Defaults to 0.001.\n",
      " |    rho: Discounting factor for the history/coming gradient. Defaults to 0.9.\n",
      " |    momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n",
      " |    epsilon: A small constant for numerical stability. This epsilon is\n",
      " |      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
      " |      1e-7.\n",
      " |    centered: Boolean. If `True`, gradients are normalized by the estimated\n",
      " |      variance of the gradient; if False, by the uncentered second moment.\n",
      " |      Setting this to `True` may help with training, but is slightly more\n",
      " |      expensive in terms of computation and memory. Defaults to `False`.\n",
      " |    name: Optional name prefix for the operations created when applying\n",
      " |      gradients. Defaults to `\"RMSprop\"`.\n",
      " |    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n",
      " |      `clipnorm`, `global_clipnorm`.\n",
      " |      If `clipvalue` (float) is set, the gradient of each weight\n",
      " |      is clipped to be no higher than this value.\n",
      " |      If `clipnorm` (float) is set, the gradient of each weight\n",
      " |      is individually clipped so that its norm is no higher than this value.\n",
      " |      If `global_clipnorm` (float) is set the gradient of all weights is\n",
      " |      clipped so that their global norm is no higher than this value.\n",
      " |  \n",
      " |  Note that in the dense implementation of this algorithm, variables and their\n",
      " |  corresponding accumulators (momentum, gradient moving average, square\n",
      " |  gradient moving average) will be updated even if the gradient is zero\n",
      " |  (i.e. accumulators will decay, momentum will be applied). The sparse\n",
      " |  implementation (used when the gradient is an `IndexedSlices` object,\n",
      " |  typically because of `tf.gather` or an embedding lookup in the forward pass)\n",
      " |  will not update variable slices or their accumulators unless those slices\n",
      " |  were used in the forward pass (nor is there an \"eventual\" correction to\n",
      " |  account for these omitted updates). This leads to more efficient updates for\n",
      " |  large embedding lookup tables (where most of the slices are not accessed in\n",
      " |  a particular graph execution), but differs from the published algorithm.\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  >>> opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
      " |  >>> var1 = tf.Variable(10.0)\n",
      " |  >>> loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1\n",
      " |  >>> step_count = opt.minimize(loss, [var1]).numpy()\n",
      " |  >>> var1.numpy()\n",
      " |  9.683772\n",
      " |  \n",
      " |  Reference:\n",
      " |    - [Hinton, 2012](\n",
      " |      http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RMSprop\n",
      " |      keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n",
      " |      Construct a new RMSprop optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
      " |          `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
      " |          that takes no arguments and returns the actual value to use. The\n",
      " |          learning rate. Defaults to 0.001.\n",
      " |        rho: Discounting factor for the history/coming gradient. Defaults to 0.9.\n",
      " |        momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n",
      " |        epsilon: A small constant for numerical stability. This epsilon is\n",
      " |          \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |          Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
      " |          1e-7.\n",
      " |        centered: Boolean. If `True`, gradients are normalized by the estimated\n",
      " |          variance of the gradient; if False, by the uncentered second moment.\n",
      " |          Setting this to `True` may help with training, but is slightly more\n",
      " |          expensive in terms of computation and memory. Defaults to `False`.\n",
      " |        name: Optional name prefix for the operations created when applying\n",
      " |          gradients. Defaults to \"RMSprop\".\n",
      " |        **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
      " |          `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
      " |          gradients by value, `decay` is included for backward compatibility to\n",
      " |          allow time inverse decay of learning rate. `lr` is included for backward\n",
      " |          compatibility, recommended to use `learning_rate` instead.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and\n",
      " |      `epsilon` can each be a callable that takes no arguments and returns the\n",
      " |      actual value to use. This can be useful for changing these values across\n",
      " |      different invocations of optimizer functions.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimizer.\n",
      " |      \n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Set the weights of the optimizer.\n",
      " |      \n",
      " |      The weights of an optimizer are its state (ie, variables).\n",
      " |      This function takes the weight values associated with this\n",
      " |      optimizer as a list of Numpy arrays. The first value is always the\n",
      " |      iterations count of the optimizer, followed by the optimizer's state\n",
      " |      variables in the order they are created. The passed values are used to set\n",
      " |      the new state of the optimizer.\n",
      " |      \n",
      " |      For example, the RMSprop optimizer for this simple model takes a list of\n",
      " |      three values-- the iteration count, followed by the root-mean-square value\n",
      " |      of the kernel and bias of the single Dense layer:\n",
      " |      \n",
      " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
      " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |      >>> m.compile(opt, loss='mse')\n",
      " |      >>> data = np.arange(100).reshape(5, 20)\n",
      " |      >>> labels = np.zeros(5)\n",
      " |      >>> results = m.fit(data, labels)  # Training.\n",
      " |      >>> new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]\n",
      " |      >>> opt.set_weights(new_weights)\n",
      " |      >>> opt.iterations\n",
      " |      <tf.Variable 'RMSprop/iter:0' shape=() dtype=int64, numpy=10>\n",
      " |      \n",
      " |      Args:\n",
      " |          weights: weight values as a list of numpy arrays.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattribute__(self, name)\n",
      " |      Overridden to support hyperparameter access.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Override setattr to support dynamic hyperparameter setting.\n",
      " |  \n",
      " |  add_slot(self, var, slot_name, initializer='zeros', shape=None)\n",
      " |      Add a new slot variable for `var`.\n",
      " |      \n",
      " |      A slot variable is an additional variable associated with `var` to train.\n",
      " |      It is allocated and managed by optimizers, e.g. `Adam`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: a `Variable` object.\n",
      " |        slot_name: name of the slot variable.\n",
      " |        initializer: initializer of the slot variable\n",
      " |        shape: (Optional) shape of the slot variable. If not set, it will default\n",
      " |        to the shape of `var`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A slot variable.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer='zeros', trainable=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>)\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      The method sums gradients from all replicas in the presence of\n",
      " |      `tf.distribute.Strategy` by default. You can aggregate gradients yourself by\n",
      " |      passing `experimental_aggregate_gradients=False`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      grads = tape.gradient(loss, vars)\n",
      " |      grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
      " |      # Processing aggregated gradients.\n",
      " |      optimizer.apply_gradients(zip(grads, vars),\n",
      " |          experimental_aggregate_gradients=False)\n",
      " |      \n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs.\n",
      " |        name: Optional name for the returned operation. Default to the name passed\n",
      " |          to the `Optimizer` constructor.\n",
      " |        experimental_aggregate_gradients: Whether to sum gradients from different\n",
      " |          replicas in the presence of `tf.distribute.Strategy`. If False, it's\n",
      " |          user responsibility to aggregate the gradients. Default to True.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. The `iterations`\n",
      " |        will be automatically increased by 1.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |        RuntimeError: If called in a cross-replica context.\n",
      " |  \n",
      " |  get_gradients(self, loss, params)\n",
      " |      Returns gradients of `loss` with respect to `params`.\n",
      " |      \n",
      " |      Should be used only in legacy v1 graph mode.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: Loss tensor.\n",
      " |        params: List of variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of gradient tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
      " |          function not implemented).\n",
      " |  \n",
      " |  get_slot(self, var, slot_name)\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      A list of names for this optimizer's slots.\n",
      " |  \n",
      " |  get_updates(self, loss, params)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the optimizer.\n",
      " |      \n",
      " |      The weights of an optimizer are its state (ie, variables).\n",
      " |      This function returns the weight values associated with this\n",
      " |      optimizer as a list of Numpy arrays. The first value is always the\n",
      " |      iterations count of the optimizer, followed by the optimizer's state\n",
      " |      variables in the order they were created. The returned list can in turn\n",
      " |      be used to load state into similarly parameterized optimizers.\n",
      " |      \n",
      " |      For example, the RMSprop optimizer for this simple model returns a list of\n",
      " |      three values-- the iteration count, followed by the root-mean-square value\n",
      " |      of the kernel and bias of the single Dense layer:\n",
      " |      \n",
      " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
      " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |      >>> m.compile(opt, loss='mse')\n",
      " |      >>> data = np.arange(100).reshape(5, 20)\n",
      " |      >>> labels = np.zeros(5)\n",
      " |      >>> results = m.fit(data, labels)  # Training.\n",
      " |      >>> len(opt.get_weights())\n",
      " |      3\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  minimize(self, loss, var_list, grad_loss=None, name=None, tape=None)\n",
      " |      Minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n",
      " |          and return the value to minimize. If a `Tensor`, the `tape` argument\n",
      " |          must be passed.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable` objects.\n",
      " |          Use callable when the variable list would otherwise be incomplete before\n",
      " |          `minimize` since the variables are created at the first time `loss` is\n",
      " |          called.\n",
      " |        grad_loss: (Optional). A `Tensor` holding the gradient computed for\n",
      " |          `loss`.\n",
      " |        name: (Optional) str. Name for the returned operation.\n",
      " |        tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n",
      " |          the tape that computed the `loss` must be provided.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that updates the variables in `var_list`. The `iterations`\n",
      " |        will be automatically increased by 1.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  variables(self)\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Creates an optimizer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same optimizer from the config\n",
      " |      dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional Python\n",
      " |            objects used to create this optimizer, such as a function used for a\n",
      " |            hyperparameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  weights\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  clipnorm\n",
      " |      `float` or `None`. If set, clips gradients to a maximum norm.\n",
      " |  \n",
      " |  clipvalue\n",
      " |      `float` or `None`. If set, clips gradients to a maximum value.\n",
      " |  \n",
      " |  global_clipnorm\n",
      " |      `float` or `None`.\n",
      " |      \n",
      " |      If set, clips gradients to a maximum norm.\n",
      " |      \n",
      " |      Check `tf.clip_by_global_norm` for more details.\n",
      " |  \n",
      " |  iterations\n",
      " |      Variable. The number of training steps this Optimizer has run.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0989f48-83a5-4635-8fc9-c2647dfea999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LHCb Analysis Facility",
   "language": "python",
   "name": "lhcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
